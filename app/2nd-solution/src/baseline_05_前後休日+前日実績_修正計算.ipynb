{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf3be50-33d3-4cf8-b0f9-a05bc987b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Library\n",
    "# ========================================\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "import jpholiday\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from scipy.optimize import minimize\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7272a17a-5dc3-404f-8c91-7f87c4035e4b",
   "metadata": {},
   "source": [
    "## データの読み込み・前加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff648373-7aa5-4d02-9cd4-fdd830a12a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_df = pd.read_csv('./train/road.csv')\n",
    "search_spec_df = pd.read_csv('./train/search_data.csv')\n",
    "search_unspec_df = pd.read_csv('./train/search_unspec_data.csv')\n",
    "train_df = pd.read_csv('./train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662e7a47-6046-4ff0-891f-ea6fa8383af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_datetime(df):\n",
    "    if 'datetime' in df.columns:\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "    if 'date' in df.columns:\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        # df[\"date\"] = df[\"date\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_dataset(train_df, search_spec_df, search_unspec_df):\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    search_spec_df['datetime'] = pd.to_datetime(search_spec_df['datetime'])\n",
    "    search_unspec_df['date'] = pd.to_datetime(search_unspec_df['date'])\n",
    "\n",
    "    train_df = expand_datetime(train_df)\n",
    "    # search_spec_df = expand_datetime(search_spec_df)\n",
    "    search_unspec_df = expand_datetime(search_unspec_df)\n",
    "\n",
    "    train_df = train_df.merge(search_spec_df, on=['datetime', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(search_unspec_df, on=['year', 'month', 'day', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(road_df.drop(['start_name', 'end_name'], axis=1), on=['start_code', 'end_code'], how='left')\n",
    "\n",
    "    train_df['dayofweek'] = train_df['datetime'].dt.weekday\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c5465-ce77-432f-ae5b-3a9733657a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = extract_dataset(train_df, search_spec_df, search_unspec_df)\n",
    "train['section'] = train['start_code'].astype(str)+'_'+train['end_code'].astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059c8138-e8fa-40e9-bebb-b22e5e31b7fe",
   "metadata": {},
   "source": [
    "## データ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ab0345-94c3-43c3-b4b8-d09b749da4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_add_a2(df, date_col=\"date\"):\n",
    "    merged_t = df.copy()\n",
    "    \n",
    "    start_year = 2021\n",
    "    end_year = 2023\n",
    "    holiday = jpholiday.between(datetime.date(start_year, 1, 1), datetime.date(end_year, 12, 31))\n",
    "    holiday_date = [x[0] for x in holiday]\n",
    "    df_holiday = pd.DataFrame(pd.date_range(start=f\"{start_year}-1-1\", end=f\"{end_year}-12-31\"),\n",
    "                              columns=[date_col]).set_index(date_col)\n",
    "    df_holiday.loc[holiday_date, \"holiday\"] = 1\n",
    "    df_holiday = df_holiday.reset_index()\n",
    "    df_holiday[\"dayofweek\"] = df_holiday[date_col].dt.weekday\n",
    "    df_holiday[\"month\"] = df_holiday[date_col].dt.month\n",
    "    df_holiday[\"day\"] = df_holiday[date_col].dt.day\n",
    "    \n",
    "    \n",
    "    # 土日も1にする\n",
    "    df_holiday.loc[df_holiday[\"dayofweek\"]==5, \"holiday\"] = 1\n",
    "    df_holiday.loc[df_holiday[\"dayofweek\"]==6, \"holiday\"] = 1\n",
    "    \n",
    "    # GW, お盆, 年末年始も1にする\n",
    "    ## GW\n",
    "    df_holiday.loc[(df_holiday[\"month\"]==4)&(df_holiday[\"day\"]>=29), \"holiday\"] = 1\n",
    "    df_holiday.loc[(df_holiday[\"month\"]==5)&(df_holiday[\"day\"]<=5), \"holiday\"] = 1\n",
    "    ## お盆\n",
    "    df_holiday.loc[(df_holiday[\"month\"]==8)&(df_holiday[\"day\"]>=12)&(df_holiday[\"day\"]<=16), \"holiday\"] = 1\n",
    "    ## 年末年始\n",
    "    df_holiday.loc[(df_holiday[\"month\"]==12)&(df_holiday[\"day\"]>=30), \"holiday\"] = 1\n",
    "    df_holiday.loc[(df_holiday[\"month\"]==1)&(df_holiday[\"day\"]<=3), \"holiday\"] = 1\n",
    "    \n",
    "    # 欠損埋め\n",
    "    df_holiday[\"holiday\"] = df_holiday[\"holiday\"].fillna(0)\n",
    "    \n",
    "    # 後休日\n",
    "    df_holiday = add_holidays_from_tomorrow(df_holiday)\n",
    "    # display(df_holiday)\n",
    "    \n",
    "    # 前休日\n",
    "    df_holiday = add_holidays_to_yesterday(df_holiday)\n",
    "    \n",
    "    \n",
    "    # 結合\n",
    "    # display(df_holiday[date_col].value_counts())\n",
    "    df_holiday[date_col] = pd.to_datetime(df_holiday[date_col]) # 型の調整\n",
    "    merged_t[date_col] = pd.to_datetime(merged_t[date_col]) # 型の調整\n",
    "    merged_t = pd.merge(merged_t, df_holiday[[date_col, \"holiday\", \"holiday_before\", \"holiday_after\"]], on=date_col, how=\"left\")\n",
    "    \n",
    "    df_holiday.to_csv(\"./calendar.csv\", index=False)\n",
    "    df_holiday.to_pickle(\"./calendar.pickle\")\n",
    "    \n",
    "    return merged_t\n",
    "\n",
    "def add_holidays_from_tomorrow(df, col_date=\"date\", col_holiday=\"holiday\"):\n",
    "    \n",
    "    # dfに追加する列名\n",
    "    col_new = 'holiday_after'\n",
    "    \n",
    "    # 日付一覧の取得\n",
    "    date_list = df[col_date]\n",
    "    \n",
    "    # 休日数の算出\n",
    "    for date in date_list:\n",
    "        # 休日数を格納\n",
    "        num_holidays = -1\n",
    "        # 休日判定\n",
    "        holidays_flag = True\n",
    "        # 次の日\n",
    "        date_next = date\n",
    "        # 休日ではなくなるまで繰り返し\n",
    "        while holidays_flag:\n",
    "            # 休日数の更新\n",
    "            num_holidays += 1\n",
    "            # 更に次の日へ\n",
    "            date_next += datetime.timedelta(days=1)\n",
    "            # 次の日のデータ取得\n",
    "            holidays = df.loc[df[col_date]==date_next, col_holiday]\n",
    "            # dfに次の日が存在しない場合(False)\n",
    "            if len(holidays) == 0:\n",
    "                holidays_flag = False\n",
    "            # 次の日が存在する場合\n",
    "            else:\n",
    "                holidays_flag = holidays.iloc[0]!=0\n",
    "        \n",
    "        # 休日数を格納\n",
    "        df.loc[df[col_date]==date, col_new] = num_holidays\n",
    "    \n",
    "    # print(df.shape, df_calendar.shape)\n",
    "    # display(df_calendar)\n",
    "    # df = pd.merge(df, df_calendar[[col_date, col_new]], on=col_date, how=\"left\")\n",
    "    # display(df)\n",
    "    # print(df.shape)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_holidays_to_yesterday(df, col_date=\"date\", col_holiday=\"holiday\"):\n",
    "    \n",
    "    # dfに追加する列名\n",
    "    col_new = 'holiday_before'\n",
    "    \n",
    "    # 日付一覧の取得\n",
    "    date_list = df[col_date]\n",
    "    \n",
    "    # 休日数の算出\n",
    "    for date in date_list:\n",
    "        # 休日数を格納\n",
    "        num_holidays = -1\n",
    "        # 休日判定\n",
    "        holidays_flag = True\n",
    "        # 次の日\n",
    "        date_next = date\n",
    "        # 休日ではなくなるまで繰り返し\n",
    "        while holidays_flag:\n",
    "            # 休日数の更新\n",
    "            num_holidays += 1\n",
    "            # 更に次の日へ\n",
    "            date_next -= datetime.timedelta(days=1)\n",
    "            # 次の日のデータ取得\n",
    "            holidays = df.loc[df[col_date]==date_next, col_holiday]\n",
    "            # dfに次の日が存在しない場合(False)\n",
    "            if len(holidays) == 0:\n",
    "                holidays_flag = False\n",
    "            # 次の日が存在する場合\n",
    "            else:\n",
    "                holidays_flag = holidays.iloc[0]!=0\n",
    "        \n",
    "        # 休日数を格納\n",
    "        df.loc[df[col_date]==date, col_new] = num_holidays\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93cc2ce8-700f-4d12-9968-99c266787854",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = holiday_add_a2(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b07043-fb53-4abc-b2d4-04dc0b045914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.read_pickle(\"./calendar.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895c1999-dacd-427e-ae62-aef4cfd8384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6546da4-0b8b-4e4b-bc35-f102b542a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[train[\"holiday_before\"]>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c54aa71-b76a-4ed5-80b6-0acf9bcc6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpholiday.between(datetime.date(2021, 1, 1), datetime.date(2023, 12, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f52d4b22-e98d-4623-acfa-00a0cb49dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"start_code\", \"end_code\", \"start_pref_code\", \"end_pref_code\", \"road_code\", \"dayofweek\", \"holiday\", \"direction\", \"month\"] #9\n",
    "# cat_cols = [\"start_pref_code\", \"end_pref_code\", \"road_code\", \"dayofweek\", \"holiday\", \"direction\", \"month\"] #9\n",
    "num_cols = [\"day\", \"search_unspec_1d\", \"start_lat\", \"end_lat\", \"start_lng\", \"end_lng\",\n",
    "            \"start_degree\", \"end_degree\", \"KP\", \"limit_speed\", \"start_KP\", \"end_KP\",\n",
    "           \"holiday_before\", \"holiday_after\"] #12\n",
    "num_1h_cols = [\"OCC\", \"allCars\", \"speed\", \"search_1h\",] #3\n",
    "rem_cols = [\"datetime\", \"year\", \"hour\"] #3\n",
    "tar_col = \"is_congestion\" #1\n",
    "key_cols = [\"date\", \"section\"] #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daefe5bb-989c-455a-9052-4a860150469b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6395e7-edc0-4b62-9f88-b77e67bf3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_for_rnn(df, cat_cols, num_cols, num_1h_cols, rem_cols, tar_col, key_cols):\n",
    "    \n",
    "    \"\"\"\n",
    "    時間固有：occ, allcars, speed, is_con(traget), hour, search_1h\n",
    "    \"\"\"\n",
    "    \n",
    "    # 例外処理\n",
    "    df[\"search_1h\"] = df[\"search_1h\"].fillna(-1)\n",
    "    df[\"search_unspec_1d\"] = df[\"search_unspec_1d\"].fillna(-1)\n",
    "        \n",
    "    # 日にち単位にまとめる\n",
    "    df_day = df[df[\"hour\"]==10] # 適当な時間\n",
    "    df_day = df_day[key_cols+cat_cols+num_cols]\n",
    "    \n",
    "    # object化\n",
    "    for col in cat_cols:\n",
    "        df_day[col] = df_day[col].astype(\"category\")\n",
    "    \n",
    "    # 時間に依存する項目のみ\n",
    "    df_1h = df.pivot_table(index=key_cols, columns=[\"hour\"], values=num_1h_cols+[tar_col], aggfunc=\"mean\")\n",
    "    col_news = [str(col[0])+\"_\"+str(col[1]) for col in df_1h.columns]\n",
    "    df_1h.columns = col_news\n",
    "    df_1h = df_1h.reset_index()\n",
    "    \n",
    "    # merge\n",
    "    df_day = pd.merge(df_day, df_1h, on=key_cols, how=\"left\")\n",
    "    \n",
    "    # one hot\n",
    "    # df_day = pd.get_dummies(df_day, drop_first=True, columns=cat_cols)\n",
    "    \n",
    "    # le\n",
    "    le_dict = {}\n",
    "    for c in tqdm(cat_cols):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df_day[c])\n",
    "        df_day[c] = le.transform(df_day[c])\n",
    "        le_dict[c] = le\n",
    "\n",
    "    with open(\"../src/features/le_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(le_dict, f)\n",
    "    \n",
    "    # sort\n",
    "    df_day = df_day.sort_values(key_cols).reset_index(drop=True)\n",
    "    \n",
    "    feature_cols = []\n",
    "    tar_cols = []\n",
    "    for col in df_day.columns:\n",
    "        if (tar_col in col)|(col in key_cols):\n",
    "            if tar_col in col:\n",
    "                tar_cols.append(col)\n",
    "        else:\n",
    "            feature_cols.append(col)\n",
    "            \n",
    "    # 目的変数を前日にする&当日の実績を変数に設定\n",
    "    df_yesterday = df_day.copy(deep=True)\n",
    "    df_yesterday[\"date\"] = df_yesterday[\"date\"] - datetime.timedelta(days=1)\n",
    "    # df_day = df_day.drop(tar_cols, axis=1) ##ここを変更する\n",
    "    old_tra_cols = [i.replace(\"is_congestion\", \"old\") for i in tar_cols]\n",
    "    feature_cols += old_tra_cols\n",
    "    df_day = df_day.rename(dict(zip(tar_cols, old_tra_cols)), axis=1)\n",
    "    df_yesterday = df_yesterday[key_cols+tar_cols]\n",
    "    df_day = pd.merge(df_day, df_yesterday, on=key_cols, how=\"left\")\n",
    "    df_day = df_day.dropna(subset=tar_cols)\n",
    "    \n",
    "    return df_day, feature_cols, tar_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daea39de-847f-47f7-9144-4d74d6a477dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df4413234a148f5a76aed9a94262597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, feature_cols, tar_cols = convert_df_for_rnn(train, cat_cols, num_cols, num_1h_cols, rem_cols, tar_col, key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0740012-d31d-4057-a9a7-521c90344b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"holiday_before\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51295f58-3b1f-490d-adae-efb34eccbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[tar_cols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "073ba93d-092f-495b-8986-9a7a8d683bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37841, 169)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7285e53f-0ce3-46d9-85ea-af1ab4c06036",
   "metadata": {},
   "source": [
    "# モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1b79c8-364d-415a-a4aa-08ee0d349fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_cols]\n",
    "Y = df[tar_cols]\n",
    "\n",
    "kf = StratifiedGroupKFold(n_splits=6)\n",
    "cv_list = list(kf.split(X, y=Y.sum(axis=1)>0, groups=X['month'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad6b618e-84f1-4cc6-9b9d-4888b0badcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(cv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7affe2e2-bcfe-45e8-955d-99d5f0f36263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     \"objective\": \"binary\",\n",
    "#     \"n_estimators\": 100000,\n",
    "#     \"learning_rate\": 0.05,\n",
    "#     \"importance_type\": \"gain\",\n",
    "#     \"random_state\": 42,\n",
    "#     \"verbose\": -1,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e644f3f-12a0-4e63-9100-6c3cc8145839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(lgb.LGBMClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44fc57cc-5953-4b70-ba1e-65f35b4fb2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b339b1b1b7a74995b8a3de7803c01090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####___Fold:00___#####\n",
      "[Chain] .................. (1 of 24) Processing order 0, total=   1.8s\n",
      "[Chain] .................. (2 of 24) Processing order 1, total=   1.9s\n",
      "[Chain] .................. (3 of 24) Processing order 2, total=   1.7s\n",
      "[Chain] .................. (4 of 24) Processing order 3, total=   1.9s\n",
      "[Chain] .................. (5 of 24) Processing order 4, total=   1.8s\n",
      "[Chain] .................. (6 of 24) Processing order 5, total=   1.8s\n",
      "[Chain] .................. (7 of 24) Processing order 6, total=   2.0s\n",
      "[Chain] .................. (8 of 24) Processing order 7, total=   2.4s\n",
      "[Chain] .................. (9 of 24) Processing order 8, total=   2.6s\n",
      "[Chain] ................. (10 of 24) Processing order 9, total=   2.8s\n",
      "[Chain] ................ (11 of 24) Processing order 10, total=   2.9s\n",
      "[Chain] ................ (12 of 24) Processing order 11, total=   2.8s\n",
      "[Chain] ................ (13 of 24) Processing order 12, total=   2.6s\n",
      "[Chain] ................ (14 of 24) Processing order 13, total=   2.3s\n",
      "[Chain] ................ (15 of 24) Processing order 14, total=   2.5s\n",
      "[Chain] ................ (16 of 24) Processing order 15, total=   2.8s\n",
      "[Chain] ................ (17 of 24) Processing order 16, total=   4.1s\n",
      "[Chain] ................ (18 of 24) Processing order 17, total=   3.5s\n",
      "[Chain] ................ (19 of 24) Processing order 18, total=   3.1s\n",
      "[Chain] ................ (20 of 24) Processing order 19, total=   2.5s\n",
      "[Chain] ................ (21 of 24) Processing order 20, total=   2.3s\n",
      "[Chain] ................ (22 of 24) Processing order 21, total=   2.2s\n",
      "[Chain] ................ (23 of 24) Processing order 22, total=   2.2s\n",
      "[Chain] ................ (24 of 24) Processing order 23, total=   2.6s\n",
      "#####___Fold:01___#####\n",
      "[Chain] .................. (1 of 24) Processing order 0, total=   1.8s\n",
      "[Chain] .................. (2 of 24) Processing order 1, total=   1.8s\n",
      "[Chain] .................. (3 of 24) Processing order 2, total=   1.7s\n",
      "[Chain] .................. (4 of 24) Processing order 3, total=   3.5s\n",
      "[Chain] .................. (5 of 24) Processing order 4, total=   4.5s\n",
      "[Chain] .................. (6 of 24) Processing order 5, total=   3.3s\n",
      "[Chain] .................. (7 of 24) Processing order 6, total=   2.1s\n",
      "[Chain] .................. (8 of 24) Processing order 7, total=   5.0s\n",
      "[Chain] .................. (9 of 24) Processing order 8, total=   2.6s\n",
      "[Chain] ................. (10 of 24) Processing order 9, total=   2.8s\n",
      "[Chain] ................ (11 of 24) Processing order 10, total=   2.9s\n",
      "[Chain] ................ (12 of 24) Processing order 11, total=   2.8s\n",
      "[Chain] ................ (13 of 24) Processing order 12, total=   2.6s\n",
      "[Chain] ................ (14 of 24) Processing order 13, total=   2.4s\n",
      "[Chain] ................ (15 of 24) Processing order 14, total=   2.5s\n",
      "[Chain] ................ (16 of 24) Processing order 15, total=   2.8s\n",
      "[Chain] ................ (17 of 24) Processing order 16, total=   3.1s\n",
      "[Chain] ................ (18 of 24) Processing order 17, total=   3.4s\n",
      "[Chain] ................ (19 of 24) Processing order 18, total=   3.2s\n",
      "[Chain] ................ (20 of 24) Processing order 19, total=   2.5s\n",
      "[Chain] ................ (21 of 24) Processing order 20, total=   2.4s\n",
      "[Chain] ................ (22 of 24) Processing order 21, total=   2.6s\n",
      "[Chain] ................ (23 of 24) Processing order 22, total=   2.2s\n",
      "[Chain] ................ (24 of 24) Processing order 23, total=   2.1s\n",
      "#####___Fold:02___#####\n",
      "[Chain] .................. (1 of 24) Processing order 0, total=   1.9s\n",
      "[Chain] .................. (2 of 24) Processing order 1, total=   1.9s\n",
      "[Chain] .................. (3 of 24) Processing order 2, total=   1.9s\n",
      "[Chain] .................. (4 of 24) Processing order 3, total=   2.0s\n",
      "[Chain] .................. (5 of 24) Processing order 4, total=   1.9s\n",
      "[Chain] .................. (6 of 24) Processing order 5, total=   1.8s\n",
      "[Chain] .................. (7 of 24) Processing order 6, total=   2.5s\n",
      "[Chain] .................. (8 of 24) Processing order 7, total=   2.7s\n",
      "[Chain] .................. (9 of 24) Processing order 8, total=   2.6s\n",
      "[Chain] ................. (10 of 24) Processing order 9, total=   2.8s\n",
      "[Chain] ................ (11 of 24) Processing order 10, total=   3.1s\n",
      "[Chain] ................ (12 of 24) Processing order 11, total=   3.0s\n",
      "[Chain] ................ (13 of 24) Processing order 12, total=   2.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m base_lr \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mLGBMClassifier(objective\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, n_estimators\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, class_weight\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, num_leaves\u001b[39m=\u001b[39m\u001b[39m63\u001b[39m, colsample_bytree\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m)\n\u001b[1;32m     14\u001b[0m clf \u001b[39m=\u001b[39m ClassifierChain(base_lr, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_state\u001b[39m=\u001b[39mi, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, Y_train)\n\u001b[1;32m     16\u001b[0m model_list\u001b[39m.\u001b[39mappend(clf)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/sklearn/multioutput.py:813\u001b[0m, in \u001b[0;36mClassifierChain.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \n\u001b[1;32m    798\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39m    Class instance.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 813\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, Y)\n\u001b[1;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m [\n\u001b[1;32m    815\u001b[0m     estimator\u001b[39m.\u001b[39mclasses_ \u001b[39mfor\u001b[39;00m chain_idx, estimator \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n\u001b[1;32m    816\u001b[0m ]\n\u001b[1;32m    817\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/sklearn/multioutput.py:632\u001b[0m, in \u001b[0;36m_BaseChain.fit\u001b[0;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[1;32m    630\u001b[0m y \u001b[39m=\u001b[39m Y[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder_[chain_idx]]\n\u001b[1;32m    631\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mChain\u001b[39m\u001b[39m\"\u001b[39m, message):\n\u001b[0;32m--> 632\u001b[0m     estimator\u001b[39m.\u001b[39;49mfit(X_aug[:, : (X\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m chain_idx)], y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m chain_idx \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    634\u001b[0m     col_idx \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m chain_idx\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[39m=\u001b[39m (valid_x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le\u001b[39m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, _y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score, eval_set\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[1;32m    968\u001b[0m             eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n\u001b[1;32m    969\u001b[0m             eval_class_weight\u001b[39m=\u001b[39;49meval_class_weight, eval_init_score\u001b[39m=\u001b[39;49meval_init_score,\n\u001b[1;32m    970\u001b[0m             eval_metric\u001b[39m=\u001b[39;49meval_metric, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m    971\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name, categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n\u001b[1;32m    972\u001b[0m             callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n\u001b[1;32m    973\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m    749\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    750\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n\u001b[1;32m    751\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n\u001b[1;32m    752\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n\u001b[1;32m    753\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n\u001b[1;32m    754\u001b[0m     fobj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fobj,\n\u001b[1;32m    755\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,\n\u001b[1;32m    756\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n\u001b[1;32m    757\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n\u001b[1;32m    758\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m    759\u001b[0m )\n\u001b[1;32m    761\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/signate3.10/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "model_list = []\n",
    "\n",
    "for i, cv in enumerate(tqdm(cv_list)):\n",
    "    print(f\"#####___Fold:{i:02}___#####\")\n",
    "    \n",
    "    ind_train, ind_valid = cv\n",
    "    X_train = X.loc[ind_train]\n",
    "    Y_train = Y.loc[ind_train]\n",
    "    X_valid = X.loc[ind_valid]\n",
    "    Y_valid = Y.loc[ind_valid]\n",
    "\n",
    "    base_lr = lgb.LGBMClassifier(objective=\"binary\", n_estimators=1000, n_jobs=-1, class_weight=\"balanced\", num_leaves=63, colsample_bytree=0.8)\n",
    "    clf = ClassifierChain(base_lr, order=None, random_state=i, verbose=2, cv=5)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    model_list.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bfa13-8297-4900-91b4-4efcf57df565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_list = []\n",
    "\n",
    "Y_pred = Y.copy(deep=True)\n",
    "for i in tqdm(range(len(model_list))):\n",
    "    print(f\"#####___Fold:{i:02}___#####\")\n",
    "    clf = model_list[i]\n",
    "    ind_train, ind_valid = cv_list[i]\n",
    "    \n",
    "    X_train = X.loc[ind_train]\n",
    "    Y_train = Y.loc[ind_train]\n",
    "    X_valid = X.loc[ind_valid]\n",
    "    Y_valid = Y.loc[ind_valid]\n",
    "    \n",
    "    Y_train_pred = clf.predict(X_train)\n",
    "    Y_valid_pred = clf.predict(X_valid)\n",
    "    \n",
    "    Y_pred.loc[ind_valid] = Y_valid_pred\n",
    "    # print(f1_score(Y_train.values.ravel(), Y_train_pred.ravel()))\n",
    "    # print(f1_score(Y_valid.values.ravel(), Y_valid_pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6c899-637f-4dd3-b68b-de04edf91e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y.values.ravel()\n",
    "y_pred = Y_pred.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaa681-bade-44bd-8da0-2c0aa66cfad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc417f4a-b40e-4ec0-ae54-7408c47fdd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix(y_true, y_pred))\n",
    "print(\"accuracy\", accuracy_score(y_true, y_pred))\n",
    "print(\"precison\", precision_score(y_true, y_pred))\n",
    "print(\"recall\", recall_score(y_true, y_pred))\n",
    "print(\"f1 score\", f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9cc9d-2e18-47aa-a2b2-a1aa92287fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(model_list):\n",
    "    with open(f'../model/lgb_fold{i}.pickle', mode=\"wb\") as f:\n",
    "        pickle.dump(m, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
