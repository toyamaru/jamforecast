{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Library\n",
    "# ========================================\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import japanize_matplotlib\n",
    "# import jpholiday\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    StratifiedKFold,\n",
    "    KFold,\n",
    "    GroupKFold,\n",
    "    StratifiedGroupKFold,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "from scipy.optimize import minimize\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from expressway_router import ExpresswayRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExpresswayRouter] Loading Road Network...\n",
      "[ExpresswayRouter] Finished.\n"
     ]
    }
   ],
   "source": [
    "i_path = '../../train/'\n",
    "o_path = '../model/'\n",
    "\n",
    "TARGET = 'is_congestion'\n",
    "\n",
    "# ExpresswayRouterを初期化する\n",
    "road_all_csv = i_path + \"road_all.csv\"\n",
    "road_local_csv = i_path + \"road_local.csv\"\n",
    "router = ExpresswayRouter(road_all_csv=road_all_csv, road_local_csv=road_local_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(i_path + 'train.csv')\n",
    "road_df = pd.read_csv(i_path + 'road_local.csv')\n",
    "search_spec_df = pd.read_csv(i_path + 'search_specified.csv')\n",
    "search_unspec_df = pd.read_csv(i_path + 'search_unspecified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドラプラデータのload\n",
    "log_pathes = glob(i_path + \"/search_raw_log/2021_04_*.csv\")\n",
    "sorted_pathes = sorted(log_pathes, key=lambda x: datetime.strptime(os.path.basename(x), '%Y_%m_%d.csv'))\n",
    "rawlog_df = pd.concat([pd.read_csv(path) for path in sorted_pathes]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_datetime(df):\n",
    "    if 'datetime' in df.columns:\n",
    "        df['year'] = df['datetime'].dt.year\n",
    "        df['month'] = df['datetime'].dt.month\n",
    "        df['day'] = df['datetime'].dt.day\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "    if 'date' in df.columns:\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset(train_df, search_spec_df, search_unspec_df):\n",
    "    train_df['datetime'] = pd.to_datetime(train_df['datetime'])\n",
    "    search_spec_df['datetime'] = pd.to_datetime(search_spec_df['datetime'])\n",
    "    search_unspec_df['date'] = pd.to_datetime(search_unspec_df['date'])\n",
    "\n",
    "    train_df = expand_datetime(train_df)\n",
    "    search_unspec_df = expand_datetime(search_unspec_df)\n",
    "\n",
    "    train_df = train_df.merge(search_spec_df, on=['datetime', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(search_unspec_df, on=['year', 'month', 'day', 'start_code', 'end_code'], how='left')\n",
    "    train_df = train_df.merge(road_df.drop(['start_name', 'end_name'], axis=1), on=['start_code', 'end_code'], how='left')\n",
    "\n",
    "    train_df['dayofweek'] = train_df['datetime'].dt.weekday\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = extract_dataset(train_df, search_spec_df, search_unspec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['section'] = train['start_code'].astype(str) + '_' + train['KP'].astype(str) + '_' + train['end_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>start_code</th>\n",
       "      <th>end_code</th>\n",
       "      <th>KP</th>\n",
       "      <th>OCC</th>\n",
       "      <th>allCars</th>\n",
       "      <th>speed</th>\n",
       "      <th>is_congestion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>search_specified</th>\n",
       "      <th>date</th>\n",
       "      <th>search_unspecified</th>\n",
       "      <th>road_code</th>\n",
       "      <th>direction</th>\n",
       "      <th>limit_speed</th>\n",
       "      <th>start_KP</th>\n",
       "      <th>end_KP</th>\n",
       "      <th>start_pref_code</th>\n",
       "      <th>end_pref_code</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>start_degree</th>\n",
       "      <th>end_degree</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-08 00:00:00</td>\n",
       "      <td>1110210</td>\n",
       "      <td>1800006</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>507</td>\n",
       "      <td>94.208661</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>下り</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>35.75582</td>\n",
       "      <td>35.80615</td>\n",
       "      <td>139.601514</td>\n",
       "      <td>139.535511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1110210_2.48_1800006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-08 01:00:00</td>\n",
       "      <td>1110210</td>\n",
       "      <td>1800006</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>444</td>\n",
       "      <td>94.469663</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>下り</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>35.75582</td>\n",
       "      <td>35.80615</td>\n",
       "      <td>139.601514</td>\n",
       "      <td>139.535511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1110210_2.48_1800006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-08 02:00:00</td>\n",
       "      <td>1110210</td>\n",
       "      <td>1800006</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>363</td>\n",
       "      <td>92.593407</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>下り</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>35.75582</td>\n",
       "      <td>35.80615</td>\n",
       "      <td>139.601514</td>\n",
       "      <td>139.535511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1110210_2.48_1800006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-08 03:00:00</td>\n",
       "      <td>1110210</td>\n",
       "      <td>1800006</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>430</td>\n",
       "      <td>94.501160</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>下り</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>35.75582</td>\n",
       "      <td>35.80615</td>\n",
       "      <td>139.601514</td>\n",
       "      <td>139.535511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1110210_2.48_1800006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-08 04:00:00</td>\n",
       "      <td>1110210</td>\n",
       "      <td>1800006</td>\n",
       "      <td>2.48</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>500</td>\n",
       "      <td>94.079840</td>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>1800</td>\n",
       "      <td>下り</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>35.75582</td>\n",
       "      <td>35.80615</td>\n",
       "      <td>139.601514</td>\n",
       "      <td>139.535511</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1110210_2.48_1800006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  start_code  end_code    KP       OCC  allCars  \\\n",
       "0 2021-04-08 00:00:00     1110210   1800006  2.48  1.833333      507   \n",
       "1 2021-04-08 01:00:00     1110210   1800006  2.48  1.750000      444   \n",
       "2 2021-04-08 02:00:00     1110210   1800006  2.48  1.500000      363   \n",
       "3 2021-04-08 03:00:00     1110210   1800006  2.48  1.583333      430   \n",
       "4 2021-04-08 04:00:00     1110210   1800006  2.48  1.750000      500   \n",
       "\n",
       "       speed  is_congestion  year  month  day  hour  search_specified  \\\n",
       "0  94.208661              0  2021      4    8     0              15.0   \n",
       "1  94.469663              0  2021      4    8     1               6.0   \n",
       "2  92.593407              0  2021      4    8     2               3.0   \n",
       "3  94.501160              0  2021      4    8     3              26.0   \n",
       "4  94.079840              0  2021      4    8     4              30.0   \n",
       "\n",
       "        date  search_unspecified  road_code direction  limit_speed  start_KP  \\\n",
       "0 2021-04-08              3419.0       1800        下り        100.0       0.8   \n",
       "1 2021-04-08              3419.0       1800        下り        100.0       0.8   \n",
       "2 2021-04-08              3419.0       1800        下り        100.0       0.8   \n",
       "3 2021-04-08              3419.0       1800        下り        100.0       0.8   \n",
       "4 2021-04-08              3419.0       1800        下り        100.0       0.8   \n",
       "\n",
       "   end_KP  start_pref_code  end_pref_code  start_lat   end_lat   start_lng  \\\n",
       "0     9.4               13             11   35.75582  35.80615  139.601514   \n",
       "1     9.4               13             11   35.75582  35.80615  139.601514   \n",
       "2     9.4               13             11   35.75582  35.80615  139.601514   \n",
       "3     9.4               13             11   35.75582  35.80615  139.601514   \n",
       "4     9.4               13             11   35.75582  35.80615  139.601514   \n",
       "\n",
       "      end_lng  start_degree  end_degree  dayofweek               section  \n",
       "0  139.535511           4.0         2.0          3  1110210_2.48_1800006  \n",
       "1  139.535511           4.0         2.0          3  1110210_2.48_1800006  \n",
       "2  139.535511           4.0         2.0          3  1110210_2.48_1800006  \n",
       "3  139.535511           4.0         2.0          3  1110210_2.48_1800006  \n",
       "4  139.535511           4.0         2.0          3  1110210_2.48_1800006  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_logs(df_log: pd.DataFrame, date: str, n_days: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    df_logに含まれる、dateからn_days日前までの検索ログのうち、dateを指定日とするレコードを抜き出す\n",
    "    '''\n",
    "    end_timestamp = pd.Timestamp(date)\n",
    "    start_timestamp = end_timestamp - pd.Timedelta(n_days, unit='day')\n",
    "    \n",
    "    df = df_log.loc[\n",
    "        (df_log.datetime >= start_timestamp) & (df_log.datetime < end_timestamp)\n",
    "    ]\n",
    "    \n",
    "    df_specified = df.loc[\n",
    "        df.spec_datetime.dt.date == end_timestamp\n",
    "    ].reset_index(drop=True)\n",
    "    return df_specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expected_passing_time_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    検索ログdfの1レコードごとに検索経路が通過する各区間の予想通過時刻を計算する\n",
    "    '''\n",
    "    start_codes = []\n",
    "    end_codes = []\n",
    "    expected_passing_times = []\n",
    "    \n",
    "    columns = ['start_code', 'end_code', 'spec_datetime', 'spec_type']\n",
    "    for (src, dest, spec_datetime, spec_type) in df.loc[:, columns].values:\n",
    "        path_with_time = router.get_route_with_time(src, dest, spec_datetime, spec_type)\n",
    "        \n",
    "        for ((start_code, passing_time), (end_code, _)) in zip(path_with_time, path_with_time[1:]):\n",
    "            start_codes.append(start_code)\n",
    "            end_codes.append(end_code)\n",
    "            expected_passing_times.append(passing_time)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'start_code': start_codes,\n",
    "        'end_code': end_codes,\n",
    "        'passing_time': expected_passing_times\n",
    "    }).astype({'start_code': 'category', 'end_code': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドラプラデータのpreprocess\n",
    "rawlog_df = rawlog_df.astype({\n",
    "    'datetime': 'datetime64[ns]',\n",
    "    'start_code': 'category',\n",
    "    'end_code': 'category',\n",
    "    # 'spec_datetime': 'datetime64[ns]',\n",
    "    'spec_type': 'category',\n",
    "    'car_type': 'category'\n",
    "    })\n",
    "rawlog_df['spec_datetime'] = pd.to_datetime(rawlog_df['spec_datetime'], format='ISO8601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_period_dict = {\n",
    "    'train': ('20210408', '20210430'), # サンプルとして1ヶ月を処理\n",
    "    # 'test': ('20230801', '20230930'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 何日前までの検索履歴を参照するか\n",
    "n_days = 7\n",
    "# 検索数の時間粒度\n",
    "sampling_rate = '1h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'passing_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'passing_time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     11\u001b[0m passing_df \u001b[38;5;241m=\u001b[39m create_expected_passing_time_dataframe(past_logs_df)\n\u001b[1;32m     13\u001b[0m _search_df \u001b[38;5;241m=\u001b[39m (passing_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassing_time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m               \u001b[38;5;241m.\u001b[39massign(search_specified\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m               \u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_code\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m               \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m g: g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_specified\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mresample(sampling_rate)\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     17\u001b[0m               \u001b[38;5;66;03m# .apply(lambda g: g.resample(sampling_rate).sum())\u001b[39;00m\n\u001b[1;32m     18\u001b[0m               \u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m---> 19\u001b[0m _search_df \u001b[38;5;241m=\u001b[39m _search_df\u001b[38;5;241m.\u001b[39mloc[\u001b[43m_search_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpassing_time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate \u001b[38;5;241m==\u001b[39m date]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m search_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([search_df, _search_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;241m.\u001b[39mdate()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m_s\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [sec]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'passing_time'"
     ]
    }
   ],
   "source": [
    "# 訓練データ期間\n",
    "start_date, end_date = data_period_dict['train']\n",
    "date_range = pd.date_range(start_date, end_date, freq='1d')\n",
    "\n",
    "search_df = pd.DataFrame()\n",
    "s = time.time()\n",
    "for date in date_range:\n",
    "    _s = time.time()\n",
    "    \n",
    "    past_logs_df = get_past_logs(rawlog_df, date, n_days=n_days)\n",
    "    passing_df = create_expected_passing_time_dataframe(past_logs_df)\n",
    "    \n",
    "    _search_df = (passing_df.set_index('passing_time')\n",
    "                  .assign(search_specified=1)\n",
    "                  .groupby(['start_code', 'end_code'])\n",
    "                  .apply(lambda g: g['search_specified'].resample(sampling_rate).sum())\n",
    "                  # .apply(lambda g: g.resample(sampling_rate).sum())\n",
    "                  .reset_index(drop=True))\n",
    "    _search_df = _search_df.loc[_search_df['passing_time'].dt.date == date].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    search_df = pd.concat([search_df, _search_df], ignore_index=True)\n",
    "    \n",
    "    print(f'{date.date()} | {time.time() - _s : .3f} [sec]')\n",
    "\n",
    "print('-'*30)\n",
    "print(f'{start_date} --> {end_date} ({len(date_range)} days) | {time.time() - s : .3f} [sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime         datetime64[ns]\n",
       "start_code             category\n",
       "end_code               category\n",
       "spec_datetime    datetime64[ns]\n",
       "spec_type              category\n",
       "car_type               category\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawlog_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = {\n",
    "    'datetime': np.datetime64,\n",
    "    'start_code': str,\n",
    "    'end_code': str,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データ\n",
    "train = pd.merge(\n",
    "    train,\n",
    "    search_df,\n",
    "    how='left',\n",
    "    left_on=['datetime', 'start_code', 'end_code'],\n",
    "    right_on=['passing_time', 'start_code', 'end_code']\n",
    ").drop('passing_time', axis=1)\n",
    "\n",
    "train['search_specified'] = train['search_specified'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['road_code', 'start_code', 'end_code', 'section', 'direction', 'hour', 'dayofweek'] # この箇所については、select_numericalあたりを使ったほうが良さそう\n",
    "num_cols = ['year', 'month', 'day', 'hour', 'search_specified', 'search_unspecified', 'KP', 'start_KP', 'end_KP', 'limit_speed', 'OCC']\n",
    "feature_cols = cat_cols + num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[feature_cols].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dict = {}\n",
    "for c in tqdm(cat_cols):\n",
    "    le = LabelEncoder()\n",
    "    train[c] = le.fit_transform(train[c])\n",
    "    le_dict[c] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X, y, cv, model_path=[], params={}, verbose=100):\n",
    "\n",
    "    models = []\n",
    "    n_records = len(X)\n",
    "    oof_pred = np.zeros((n_records), dtype=np.float32)\n",
    "\n",
    "    def objective(trial):\n",
    "        lgb_params = {\n",
    "            \"objective\": \"binary\",\n",
    "            \"metric\": \"binary_logloss\",\n",
    "            \"boosting_type\": \"rf\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boost_from_average\": \"false\",\n",
    "            \"random_seed\": 42,\n",
    "            \"feature_pre_filter\": False,\n",
    "            \"max_depth\": trial.suggest_int('max_depth', 4, 8),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 100),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.1, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.1, 1.0),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 25),\n",
    "            \"min_data_in_leaf\": trial.suggest_int('min_data_in_leaf', 1, 4)\n",
    "        }\n",
    "\n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                  eval_set=[(x_valid, y_valid)],\n",
    "                  callbacks=[\n",
    "                      lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "                      lgb.log_evaluation(100)\n",
    "                  ]\n",
    "                  )\n",
    "\n",
    "        pred_y = model.predict_proba(x_valid)[:, 1]\n",
    "        auc = roc_auc_score(y_valid, pred_y)\n",
    "\n",
    "        return auc\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv):\n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=10)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        clf = lgb.LGBMClassifier(**best_params)\n",
    "\n",
    "        clf.fit(x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "                    lgb.log_evaluation(100)\n",
    "                ]\n",
    "                )\n",
    "\n",
    "        pred_i = clf.predict_proba(x_valid)[:, 1]\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        score = roc_auc_score(y_valid, pred_i)\n",
    "        print(f\" - fold{i + 1} - {score:.4f}\")\n",
    "\n",
    "    score = roc_auc_score(y, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISH: CV Score: {score:.4f}\")\n",
    "    return score, oof_pred, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLIT = 5\n",
    "kf = StratifiedGroupKFold(N_SPLIT)\n",
    "cv_list = list(kf.split(train, y=train[TARGET], groups=train['date']))\n",
    "\n",
    "X = train[feature_cols].values\n",
    "y = train[TARGET].values\n",
    "\n",
    "print('train shape:', train.shape)\n",
    "\n",
    "# training\n",
    "score, oof_pred, models = train_lgbm(X, y=y, cv=cv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適な閾値を探索\n",
    "\n",
    "def func(x_list, df, oof):\n",
    "    score = f1_score(df[TARGET], oof>x_list[0])\n",
    "    return -score\n",
    "\n",
    "x0 = [0.5]\n",
    "result = minimize(func, x0,  args=(train, oof_pred), method=\"nelder-mead\")\n",
    "threshold = result.x[0]\n",
    "train['pred'] = (oof_pred>threshold).astype(int)\n",
    "print('threshold:', threshold)\n",
    "print(classification_report(train[TARGET], train['pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/model.pickle', mode='wb') as f:\n",
    "    pickle.dump(models,f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# feature importance\n",
    "# ========================================\n",
    "def visualize_importance(models, feat_train_df):\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    for i, model in enumerate(models):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[\"feature_importance\"] = model.feature_importances_\n",
    "        _df[\"column\"] = feat_train_df.columns\n",
    "        _df[\"fold\"] = i + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, _df],\n",
    "                                          axis=0, ignore_index=True)\n",
    "\n",
    "    order = feature_importance_df.groupby(\"column\")\\\n",
    "        .sum()[[\"feature_importance\"]]\\\n",
    "        .sort_values(\"feature_importance\", ascending=False).index\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(order) * .25)))\n",
    "    sns.boxplot(data=feature_importance_df,\n",
    "                  x=\"feature_importance\",\n",
    "                  y=\"column\",\n",
    "                  order=order,\n",
    "                  ax=ax,\n",
    "                  palette=\"viridis\",\n",
    "                  orient=\"h\")\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    ax.set_title(\"Importance\")\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    return fig, ax, feature_importance_df\n",
    "\n",
    "fig, ax, feature_importance_df = visualize_importance(models, train[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
